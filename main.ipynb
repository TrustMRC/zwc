{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-10-27T15:30:31.189913Z",
     "iopub.status.busy": "2022-10-27T15:30:31.189314Z",
     "iopub.status.idle": "2022-10-27T15:33:27.590641Z",
     "shell.execute_reply": "2022-10-27T15:33:27.589351Z",
     "shell.execute_reply.started": "2022-10-27T15:30:31.189880Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlepaddle-gpu==2.3.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cd/21/ab9c6a282615021e3099092a04077d6db66b5ae5756d9765e9bd62c0c54b/paddlepaddle_gpu-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (394.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.0/394.0 MB\u001b[0m \u001b[31m975.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (8.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (4.4.2)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (3.3.0)\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (0.1.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (2.24.0)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (3.20.0)\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.3.2) (0.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.3.2) (1.25.6)\n",
      "Installing collected packages: paddlepaddle-gpu\n",
      "  Attempting uninstall: paddlepaddle-gpu\n",
      "    Found existing installation: paddlepaddle-gpu 2.3.2.post112\n",
      "    Uninstalling paddlepaddle-gpu-2.3.2.post112:\n",
      "      Successfully uninstalled paddlepaddle-gpu-2.3.2.post112\n",
      "Successfully installed paddlepaddle-gpu-2.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlenlp==2.3.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/e1/94cdbaca400a57687a8529213776468f003b64b6e35a6f4acf6b6539f543/paddlenlp-2.3.4-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7c/75/d282907e7ebd87e4b3475bc5156140465372fa451bc6cbddbefa54915d00/datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.9/441.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (1.0.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.1.96)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (1.2.2)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.42.1)\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (1.0.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (4.1.0)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.3.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (4.27.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.4.4)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (3.20.0)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.70.11.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (2.24.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (5.1.2)\n",
      "Collecting tqdm\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m768.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (4.2.0)\n",
      "Collecting xxhash\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fc/ca/8dd0bcbd592d6f355b75c473f5eedc6b432ffad74ad7c36e16de6bbe68fd/xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m558.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/08/9b5fe7c9e2774bca77dae29d22a446ead804fb8e050f2899ae1f60d73ad1/pyarrow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (1.1.5)\n",
      "Collecting aiohttp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/48/7882af39221fee58e33eee6c8e516097e2331334a5937f54fe5b5b285d9e/aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (1.19.5)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b2/2b/715a1924d470691a27b2dcdf472a9ef87f04718a897de25e68bf86ac0184/huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m646.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4c/3d/86112db0fc482c4b11031516340e2d9978aa837ee104b4bebaaad0fae465/fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paddlefsl\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.3.4) (0.24.2)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (4.3.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (22.1.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/51/a507c856293ab05cdc1db77ff4bc1268ddd39f29e7dc4919aa497f0adbec/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/3f/1c876ed190e8fcd1a2faef3085427e5465076e28813a2499502633f7eed3/multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/b2/cf7e86583f03fafc93c4103f9a03aaf729dcf4dca9cd3012256a48b766ad/frozenlist-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/89/36a50cab1be3d5099ec66a41212cf0c11507c343074e97e907a2f5f1a569/yarl-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (2019.9.11)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (0.14.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.3.4) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.4) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.4) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=2.0.0->paddlenlp==2.3.4) (1.16.0)\n",
      "Installing collected packages: xxhash, urllib3, tqdm, pyarrow, multidict, fsspec, frozenlist, charset-normalizer, asynctest, async-timeout, yarl, aiosignal, responses, paddlefsl, huggingface-hub, aiohttp, datasets, paddlenlp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.6\n",
      "    Uninstalling urllib3-1.25.6:\n",
      "      Successfully uninstalled urllib3-1.25.6\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.27.0\n",
      "    Uninstalling tqdm-4.27.0:\n",
      "      Successfully uninstalled tqdm-4.27.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 2.0.0\n",
      "    Uninstalling pyarrow-2.0.0:\n",
      "      Successfully uninstalled pyarrow-2.0.0\n",
      "  Attempting uninstall: paddlefsl\n",
      "    Found existing installation: paddlefsl 1.0.0\n",
      "    Uninstalling paddlefsl-1.0.0:\n",
      "      Successfully uninstalled paddlefsl-1.0.0\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.1.1\n",
      "    Uninstalling paddlenlp-2.1.1:\n",
      "      Successfully uninstalled paddlenlp-2.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parl 1.4.1 requires pyzmq==18.1.1, but you have pyzmq 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.1.1 datasets-2.6.1 frozenlist-1.3.1 fsspec-2022.10.0 huggingface-hub-0.10.1 multidict-6.0.2 paddlefsl-1.1.0 paddlenlp-2.3.4 pyarrow-9.0.0 responses-0.18.0 tqdm-4.64.1 urllib3-1.25.11 xxhash-3.1.0 yarl-1.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting trustai==0.1.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/7d/30695427906c3a820dc44f6f9563f467d34f2df7ee0502df29c7bb224bb6/trustai-0.1.5-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (0.24.2)\n",
      "Requirement already satisfied: IPython in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (7.34.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (2.0.10)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (5.4.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (4.4.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.17.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (56.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (4.7.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (2.13.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (2.8.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (3.0.9)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (2.1.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jedi>=0.16->IPython->trustai==0.1.5) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pexpect>4.3->IPython->trustai==0.1.5) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->trustai==0.1.5) (0.1.7)\n",
      "Installing collected packages: trustai\n",
      "Successfully installed trustai-0.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U paddlepaddle-gpu==2.3.2\n",
    "!pip3 install -U paddlenlp==2.3.4\n",
    "!pip3 install trustai==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T15:55:17.310815Z",
     "iopub.status.busy": "2022-10-27T15:55:17.310245Z",
     "iopub.status.idle": "2022-10-27T15:55:27.732521Z",
     "shell.execute_reply": "2022-10-27T15:55:27.731340Z",
     "shell.execute_reply.started": "2022-10-27T15:55:17.310786Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-27 23:55:19,865] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams\n",
      "W1027 23:55:19.869297  3198 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.2\n",
      "W1027 23:55:19.872598  3198 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n",
      "[2022-10-27 23:55:24,946] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt\n",
      "[2022-10-27 23:55:24,971] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json\n",
      "[2022-10-27 23:55:24,973] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "from paddlenlp.data import JiebaTokenizer\n",
    "from paddlenlp.transformers import ErnieForQuestionAnswering, ErnieTokenizer\n",
    "from utils import *\n",
    "from mrc_utils import *\n",
    "paddle.set_device(\"gpu\")\n",
    "\n",
    "# Select pre-trained model\n",
    "MODEL_NAME = \"ernie-3.0-xbase-zh\" # choose from [\"ernie-1.0\", \"ernie-1.0-base-zh\", \"ernie-1.0-large-zh-cw\", \"ernie-2.0-base-zh\", \"ernie-2.0-large-zh\", \"ernie-3.0-xbase-zh\", \"ernie-3.0-base-zh\", \"ernie-3.0-medium-zh\", \"ernie-3.0-mini-zh\", \"ernie-3.0-micro-zh\", \"ernie-3.0-nano-zh\"]\n",
    "# Select dataset for model training\n",
    "DATASET_NAME = 'dureader_robust'\n",
    "# Set the path to save the trained model\n",
    "MODEL_SAVE_PATH = f'save_model/{DATASET_NAME}-{MODEL_NAME}'\n",
    "# Set the rationale length ratio which determines the length of the extracted rationales.\n",
    "RATIONALE_RATIO = 0.096 # 0.096 for Chinese dataset, 0.102 for English dataset\n",
    "\n",
    "# Init model and tokenizer\n",
    "model = ErnieForQuestionAnswering.from_pretrained(MODEL_NAME, num_classes=2)\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Correct MODEL_PATH and DATA_PATH before executing\n",
    "MODEL_PATH = MODEL_SAVE_PATH + '/model_state.pdparams'\n",
    "\n",
    "# Load the trained parameters\n",
    "state_dict = paddle.load(MODEL_PATH)\n",
    "model.set_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS--WRITED--BY--MYSELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T15:55:27.886404Z",
     "iopub.status.busy": "2022-10-27T15:55:27.886014Z",
     "iopub.status.idle": "2022-10-27T15:55:27.927792Z",
     "shell.execute_reply": "2022-10-27T15:55:27.927065Z",
     "shell.execute_reply.started": "2022-10-27T15:55:27.886381Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "WORK_ROOT = 'work/version1.2/'\n",
    "DATA_PATH = WORK_ROOT + 'data/'\n",
    "RESULT_PATH = WORK_ROOT + 'data/result/'\n",
    "if not os.path.exists(WORK_ROOT):\n",
    "    os.makedirs(WORK_ROOT)\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "# 文本过滤后答案的丢失率\n",
    "def answers_lost_rate(dataset):\n",
    "    count = 0\n",
    "    for example in dataset:\n",
    "        for answer in example['answers']:\n",
    "            if example['context'].count(answer):\n",
    "                count += 1\n",
    "                break\n",
    "    rate = (len(dataset)-count) / len(dataset)\n",
    "    return rate\n",
    "\n",
    "# 文本过滤后的文本删除率\n",
    "def rm_redundency_rate(original, preprocessed):\n",
    "    original_count = 0\n",
    "    preprocessed_count = 0\n",
    "    for example in original:\n",
    "        original_count += len(example['context'])\n",
    "    for example in preprocessed:\n",
    "        preprocessed_count += len(example['context'])\n",
    "\n",
    "    rate = (original_count-preprocessed_count) / original_count\n",
    "    return rate\n",
    "\n",
    "# 自定义的文本过滤器\n",
    "def further_select(dataset, data_type, threshold, rm_inte=True):\n",
    "    # context，question抽取出来存储到documents和queries(List)\n",
    "    documents = []\n",
    "    queries = []\n",
    "    for data in dataset:\n",
    "        documents.append([data['context']])\n",
    "        queries.append([data['question']])\n",
    "    assert len(documents) == len(queries)\n",
    "\n",
    "    # 对document中的每一个document进行标点符号的切分，将其分成一句一句的形式，以句号级别为分割，剔除疑问句\n",
    "    max_sentence_num = -1\n",
    "    min_sentence_num = 1000\n",
    "    avg_sentence_num = 0\n",
    "    documents_split = [re.split(r'[。！!|]', document[0].strip()) for document in documents]\n",
    "    \n",
    "    for document in documents_split:\n",
    "        if len(document) > max_sentence_num:\n",
    "            max_sentence_num = len(document)\n",
    "        if len(document) < min_sentence_num:\n",
    "            min_sentence_num = len(document)\n",
    "        avg_sentence_num += len(document)\n",
    "    avg_sentence_num /= len(dataset)\n",
    "    \n",
    "    print('该数据集中最长文档包含句数：', max_sentence_num, '\\n该数据集中最短文档包含句数：', min_sentence_num, '\\n该数据集中文档平均包含句数：', avg_sentence_num)\n",
    "\n",
    "    if rm_inte:\n",
    "        interrogative_words = set(['谁','何','么','哪','几时','几个','几十','几百','几千','几万','几亿','多少','怎','吗','呢','难道','岂','究竟','?','？','行不行','好不好','能不能','要不要'])\n",
    "    else:\n",
    "        interrogative_words = set([])\n",
    "\n",
    "    documents_split_rm_inte = []\n",
    "    for document in documents_split:\n",
    "        d = []\n",
    "        for sentences in document:\n",
    "            '''\n",
    "            if sentences != \"\":\n",
    "                se = []\n",
    "                sentence = re.split(r'[,，:：;； ]', sentences.strip())\n",
    "                for s in sentence:\n",
    "                    if s != \"\" and len(set(s)&interrogative_words) == 0:\n",
    "                        se.append(s)\n",
    "                if se != []:\n",
    "                    d.append([\",\".join(se)])\n",
    "            '''\n",
    "            if sentences != \"\" and all([word not in sentences for word in interrogative_words]):\n",
    "                d.append([sentences])\n",
    "        if d != []:\n",
    "            documents_split_rm_inte.append(d)\n",
    "        else:\n",
    "            documents_split_rm_inte.append([[sentence] for sentence in document if sentence != \"\"])\n",
    "\n",
    "    # 导入词向量及分词模型\n",
    "    token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "    tokenizer = JiebaTokenizer(vocab=token_embedding.vocab)\n",
    "    embedding = token_embedding.search(\"中国\")\n",
    "\n",
    "    # 对documents实现从自然语言 -> vector操作\n",
    "    pbar = tqdm(total=len(documents_split_rm_inte), ncols=100, desc='已操作文档数目：')\n",
    "    documents_vector = []\n",
    "\n",
    "    for document in documents_split_rm_inte:  # 针对每一文档一一操作\n",
    "        pbar.update(1)\n",
    "        document_vector = []\n",
    "        for sentence in document:  # 针对文档的每一句进行一一操作\n",
    "            words = tokenizer.cut(sentence[0])\n",
    "            word_embeds = np.zeros(embedding.shape)\n",
    "            for word in words:\n",
    "                word_embed = token_embedding.search(word)  # 句子分词\n",
    "                word_embeds = np.append(word_embeds, word_embed, axis=0)\n",
    "            word_embeds = np.delete(word_embeds, 0, axis=0)\n",
    "            sentence_embed = np.mean(word_embeds, axis=0).tolist()  # 均值化词向量用于表示句向量\n",
    "            assert len(sentence_embed) == 300\n",
    "            document_vector.append(sentence_embed)\n",
    "        if len(document_vector) < max_sentence_num:  # 填充文档长度一边快速计算\n",
    "            need_padding_line_nums = max_sentence_num - len(document_vector)\n",
    "            paddings = [[0]*len(sentence_embed) for i in range(need_padding_line_nums)]\n",
    "            document_vector.extend(paddings)\n",
    "        assert len(document_vector) == max_sentence_num\n",
    "        documents_vector.append(document_vector)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    documents_tensor = paddle.to_tensor(documents_vector)\n",
    "\n",
    "    # 实现queries从自然语言-> vector操作\n",
    "    pbar = tqdm(total=len(documents_split_rm_inte), ncols=100, desc='已操作查询数目：')\n",
    "    queries_vector = []\n",
    "\n",
    "    for query in queries:\n",
    "        pbar.update(1)\n",
    "        words = tokenizer.cut(query[0])\n",
    "        word_embeds = np.zeros(embedding.shape)\n",
    "        for word in words:\n",
    "            word_embed = token_embedding.search(word)\n",
    "            word_embeds = np.append(word_embeds, word_embed, axis=0)\n",
    "        word_embeds = np.delete(word_embeds, 0, axis=0)\n",
    "        sentence_embed = np.mean(word_embeds, axis=0).tolist()  # 均值化词向量用于表示句向量\n",
    "        assert len(sentence_embed) == 300\n",
    "        queries_vector.append([sentence_embed])\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    queries_tensor = paddle.to_tensor(queries_vector)\n",
    "\n",
    "    # 计算分数矩阵并获取相似度的高的句子索引\n",
    "    score_matrix = documents_tensor * queries_tensor\n",
    "    score = score_matrix.sum(axis=-1)\n",
    "    sign = -paddle.cast(score<0, dtype='float')\n",
    "    sign[sign==0] = 1\n",
    "    # print(sign[534])\n",
    "    score *= sign\n",
    "\n",
    "    # print(score[:2])\n",
    "    index_min_to_max = paddle.argsort(score)\n",
    "    index_max_to_min = [i[::-1] for i in index_min_to_max]\n",
    "    # print(index_max_to_min[:2])\n",
    "\n",
    "    max_value = score.max()\n",
    "    score[score==0.0] = max_value + 1.0\n",
    "    min_score = score.min(axis=1).unsqueeze(1)\n",
    "    score[score==max_value+1.0] = 0.0\n",
    "    # print(min_score[:2])\n",
    "    \n",
    "    score -= min_score * 4/5\n",
    "    score[score<0] = 0.0\n",
    "    sum_score = score.sum(axis=1).unsqueeze(1)\n",
    "    # print(sum_score[:2])\n",
    "    # print(score[:2])\n",
    "\n",
    "    # 根据score选取topK个关键句(topK按占比进行选择)\n",
    "    documents_split_topK = []\n",
    "    for idx, document in enumerate(documents_split_rm_inte):\n",
    "        # print(idx, document)\n",
    "        index_sorted = index_max_to_min[idx][:len(score[idx].nonzero())]\n",
    "        if len(index_sorted) <= 1:\n",
    "            documents_split_topK.append(document)\n",
    "            continue\n",
    "        index_original = []\n",
    "        ratio = 0.0\n",
    "        d = []\n",
    "        # print(index_sorted)\n",
    "        for index in index_sorted:\n",
    "            ratio += score[idx][index] / sum_score[idx]\n",
    "            # print(ratio)\n",
    "            index_original.append(index)\n",
    "            if ratio >= threshold:\n",
    "                break\n",
    "        # print(index_original)\n",
    "        index_original = sorted(index_original)\n",
    "        # print(index_original)\n",
    "        # print('**** ****')\n",
    "        # print(index_original)\n",
    "        for index in index_original:\n",
    "            d.append(document[index])\n",
    "        documents_split_topK.append(d)\n",
    "    \n",
    "\n",
    "    # 将关键句子按原序重组\n",
    "    documents_split_topK_merge = [[sentence[0] for sentence in document] for document in documents_split_topK]\n",
    "    documents_split_topK_merge = [\"。\".join(document)+'。' for document in documents_split_topK_merge]\n",
    "\n",
    "    # 利用关键句构造一个新的json训练集\n",
    "    # 由于contex被改变，answer的position也应当被改变（针对训练集）\n",
    "    dataset_new = copy.deepcopy(dataset)\n",
    "    assert dataset_new != dataset\n",
    "    for i in range(len(dataset_new)):\n",
    "        dataset_new[i]['context'] = documents_split_topK_merge[i].strip()\n",
    "        if data_type == 'train':\n",
    "            dataset_new[i]['answer_starts'][0] = documents_split_topK_merge[i].find(dataset_new[i]['answers'][0])\n",
    "        if data_type == 'test':\n",
    "            dataset_new[i]['sent_token'] = [w for w in dataset_new[i]['context']]\n",
    "\n",
    "    return dataset_new\n",
    "\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "def context_filter(dataset, filter_type, data_type, file_name):\n",
    "    assert filter_type == 0 or (filter_type != 0 and file_name)\n",
    "    dataset_filter = copy.deepcopy(dataset)\n",
    "    preprocess = ['None', 'select', 'further_select', 'add_further_select']\n",
    "    if preprocess[filter_type] == 'None':\n",
    "        pass\n",
    "    elif preprocess[filter_type] == 'select':\n",
    "        with open(DATA_PATH + file_name) as f:\n",
    "            dataset_select = json.load(f)\n",
    "        for idx, example in enumerate(dataset_select['data'][0]['paragraphs']):\n",
    "            dataset_filter[idx]['context'] = example['context']\n",
    "            dataset_filter[idx]['sent_token'] = [word for word in example['context']]\n",
    "    elif preprocess[filter_type] == 'further_select':\n",
    "        with open(DATA_PATH + file_name) as f:\n",
    "            dataset_select = json.load(f)\n",
    "        for idx, example in enumerate(dataset_select['data'][0]['paragraphs']):\n",
    "            dataset_filter[idx]['context'] = example['context']\n",
    "        threshold = float(input('请输入further select句子分数之和的阈值'))\n",
    "        THRESHOLD = threshold\n",
    "        dataset_filter = further_select(dataset_filter, data_type, threshold)\n",
    "    elif preprocess[filter_type] == 'add_further_select':\n",
    "        data_file = DATA_PATH + file_name\n",
    "        dataset_further_select = DuReader().read(data_file)\n",
    "        for idx, example in enumerate(dataset_further_select):\n",
    "            if dataset_filter[idx]['sent_token'][-1] not in '。？！?!':\n",
    "                dataset_filter[idx]['context'] += '。' + example['context']\n",
    "            else:\n",
    "                dataset_filter[idx]['context'] += example['context']\n",
    "            dataset_filter[idx]['context'] = dataset_filter[idx]['context']\n",
    "            dataset_filter[idx]['sent_token'] = [word for word in dataset_filter[idx]['context']]\n",
    "    \n",
    "    if 'test' not in file_name:\n",
    "        print('answer loss rate:', answers_lost_rate(dataset_filter))\n",
    "    print('remove redundency rate:', rm_redundency_rate(dataset, dataset_filter))\n",
    "\n",
    "    return dataset_filter\n",
    "\n",
    "\n",
    "# 保存测试数据\n",
    "def save_test_data(dataset, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for example in dataset:\n",
    "            s = json.dumps(example, ensure_ascii=False)\n",
    "            f.writelines(s+'\\n')\n",
    "\n",
    "# 载入测试数据\n",
    "def load_test_data(data_file):\n",
    "    if os.path.exists(data_file):\n",
    "        data_ds = DuReader().read(data_file)\n",
    "        data = load_data(data_file)\n",
    "    else:\n",
    "        data_path = DATA_PATH + 'test.txt'\n",
    "        data_ds = DuReader().read(data_path)\n",
    "        if data_file.split('/')[-1].count('test_select'):\n",
    "            data_ds = context_filter(data_ds, 1, 'test', 'test_select.json')\n",
    "        elif data_file.split('/')[-1].count('test_further_select'):\n",
    "            data_ds = context_filter(data_ds, 2, 'test', 'test_select.json')\n",
    "        elif data_file.split('/')[-1].count('test_add_further_select'):\n",
    "            data_ds = context_filter(data_ds, 3, 'test', 'test_further_select.txt')\n",
    "        \n",
    "        save_test_data(data_ds, data_file)\n",
    "        data = load_data(data_file)\n",
    "    \n",
    "    print(\"Num of data:\", len(data))\n",
    "\n",
    "    return data_ds, data\n",
    "\n",
    "\n",
    "\n",
    "def cal_sent_pos(dataset):\n",
    "    context_split = []\n",
    "    context_sent_pos = []\n",
    "    for example in dataset:\n",
    "        context_split.append([s for s in re.split(r'[。，；！？,;!?]', example['context'].strip()) if s != \"\"])\n",
    "        sent_num = len(context_split[-1])\n",
    "        sent_pos = [0]\n",
    "        end_pos = 0\n",
    "        for word in example['context']:  # 英文单词有拆分\n",
    "            if word not in '。，；！？,;!?':\n",
    "                end_pos += 1\n",
    "            else:\n",
    "                end_pos += 1  # 包含标点符号\n",
    "                sent_pos.append(end_pos)\n",
    "        if len(sent_pos) == sent_num:  # 确保包含头尾\n",
    "            sent_pos.append(end_pos+1)\n",
    "        context_sent_pos.append(sent_pos)\n",
    "    \n",
    "    print(dataset[0], '\\n', context_sent_pos[0])\n",
    "    \n",
    "    return context_sent_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### 2）数据预处理\n",
    "\n",
    "a) 输入格式化：将输入的两个文本组织成模型预测所需格式，如对于Ernie3.0-base模型，其输入形式为[CLS]question[SEP]context[SEP]\n",
    "\n",
    "b) 分词位置索引：计算每个分词结果对应的原文位置索引，这里的分词包括模型分词和标准分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T15:55:27.929548Z",
     "iopub.status.busy": "2022-10-27T15:55:27.929214Z",
     "iopub.status.idle": "2022-10-27T15:55:28.020147Z",
     "shell.execute_reply": "2022-10-27T15:55:28.019458Z",
     "shell.execute_reply.started": "2022-10-27T15:55:27.929523Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data: 1855\n",
      "{'id': 1, 'context': '地瓜不是红薯。红薯是粗粮，也叫番薯山芋。', 'question': '地瓜和红薯一样吗', 'sent_token': ['地', '瓜', '不', '是', '红', '薯', '。', '红', '薯', '是', '粗', '粮', '，', '也', '叫', '番', '薯', '山', '芋', '。']} \n",
      " [0, 7, 13, 20]\n"
     ]
    }
   ],
   "source": [
    "# DATA_FILE = DATA_PATH + 'test.txt'  # version 1.0\n",
    "# DATA_FILE = DATA_PATH + 'test_select.txt'  # version 1.1\n",
    "DATA_FILE = DATA_PATH + 'test_further_select.txt'  # version 1.2\n",
    "# DATA_FILE = DATA_PATH + 'test_add_further_select.txt'  # version 1.3\n",
    "\n",
    "data_ds, data = load_test_data(DATA_FILE)\n",
    "context_sent_pos = cal_sent_pos(data_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T05:16:41.856055Z",
     "iopub.status.busy": "2022-10-27T05:16:41.854821Z",
     "iopub.status.idle": "2022-10-27T05:16:50.203269Z",
     "shell.execute_reply": "2022-10-27T05:16:50.202144Z",
     "shell.execute_reply.started": "2022-10-27T05:16:41.856009Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855\n",
      "1855\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from mrc_utils import *\n",
    "# Hyperparameters\n",
    "batch_size = 12\n",
    "max_seq_length = 512\n",
    "epochs = 3  #3\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "doc_stride = 512\n",
    "\n",
    "data_ds_new = copy.deepcopy(data_ds)\n",
    "\n",
    "# Prepare dataloader\n",
    "test_trans_func = partial(prepare_validation_features, \n",
    "                            max_seq_length=max_seq_length, \n",
    "                            doc_stride=doc_stride,\n",
    "                            tokenizer=tokenizer)\n",
    "print(len(data_ds_new))\n",
    "                            \n",
    "data_ds_new.map(test_trans_func, batched=True, num_workers=4)\n",
    "test_batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "        data_ds_new, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(len(data_ds_new))\n",
    "\n",
    "test_batchify_fn = lambda samples, fn=Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\n",
    "}): fn(samples)\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=data_ds_new,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=test_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "# Get offset maps which will be used for score alignment\n",
    "contexts, standard_split, ori_offset_maps, standard_split_offset_maps = pre_process(data, data_ds_new, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3）重要度分数获取\n",
    "我们提供attention和IG两种解释方法，可根据实际实验结果选取最有效的一种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a） Attention-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T05:16:50.231180Z",
     "iopub.status.busy": "2022-10-27T05:16:50.230568Z",
     "iopub.status.idle": "2022-10-27T05:17:28.709771Z",
     "shell.execute_reply": "2022-10-27T05:17:28.708833Z",
     "shell.execute_reply.started": "2022-10-27T05:16:50.231152Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level import AttentionInterpreter\n",
    "from utils import create_dataloader_from_scratch\n",
    "import paddle\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Init an attention interpreter and get the importance scores\n",
    "att = AttentionInterpreter(model, device=\"gpu\", predict_fn=attention_predict_fn)\n",
    "\n",
    "# Use attention interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in test_data_loader:\n",
    "    if interp_results:\n",
    "        interp_results += att(batch)\n",
    "    else:\n",
    "        interp_results = att(batch)\n",
    "\n",
    "# Trim the output to get scores only for context\n",
    "interp_results = trim_output(interp_results, data_ds_new, tokenizer)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = att.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) erasable-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T05:17:28.714647Z",
     "iopub.status.busy": "2022-10-27T05:17:28.714173Z",
     "iopub.status.idle": "2022-10-27T05:17:28.796007Z",
     "shell.execute_reply": "2022-10-27T05:17:28.794920Z",
     "shell.execute_reply.started": "2022-10-27T05:17:28.714595Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import pickle as pkl\n",
    "import math\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "test_batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "        data_ds_new, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_batchify_fn = lambda samples, fn=Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\n",
    "}): fn(samples)\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=data_ds_new,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=test_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "def MSE(y,t):\n",
    "    #形参t代表训练数据（监督数据）（真实）\n",
    "    #y代表预测数据\n",
    "    return 0.5*np.sum((y-t)**2)\n",
    "\n",
    "# 避免单词训练耗时，训练完保存\n",
    "def save_word_loss(word_loss, file_path):\n",
    "    all_loss = {}\n",
    "    idx = 0\n",
    "    for loss in word_loss:\n",
    "        d = {}\n",
    "        for l in loss:\n",
    "            key, value = l\n",
    "            d[key] = value\n",
    "        all_loss[idx] = d\n",
    "        idx += 1\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pkl.dump(all_loss, f)\n",
    "\n",
    "WORD_LOSS_FILE = DATA_PATH + 'word_loss.pkl'\n",
    "word_loss = []\n",
    "if os.path.exists(WORD_LOSS_FILE):\n",
    "    with open(WORD_LOSS_FILE, 'rb') as f:\n",
    "        word_loss_dict = pkl.load(f)\n",
    "    for key in word_loss_dict:\n",
    "        term = zip(word_loss_dict[key].keys(), word_loss_dict[key].values())\n",
    "        loss = []\n",
    "        for t in term:\n",
    "            loss.append(t)\n",
    "        word_loss.append(loss)\n",
    "else:\n",
    "    pbar = tqdm(total=len(data)/batch_size, ncols=100, desc='已操作文档数目：')\n",
    "\n",
    "    for step, batch in enumerate(test_data_loader, start=0):\n",
    "        pbar.update(1)\n",
    "        data_id=data_ds[step]['id']\n",
    "        results=align_res[step]\n",
    "        topk = math.ceil(len(data[data_id]['sent_token'])*RATIONALE_RATIO)\n",
    "        test_context=data[data_id]\n",
    "        context=test_context['context']\n",
    "        seg_list =pseg.cut(context)\n",
    "        ner_list=[]\n",
    "        for w in seg_list:\n",
    "            ner_list.append(w)\n",
    "        # print(ner_list)\n",
    "        # print(dev_question[step])\n",
    "        # global_step += 1\n",
    "        input_ids, segment_ids= batch\n",
    "        real_logits = model(input_ids=input_ids, token_type_ids=segment_ids)\n",
    "        start_logits,end_logits=real_logits\n",
    "        # print(start_logits.numpy())\n",
    "        # print('\\n',end_logits)\n",
    "        input_ids=input_ids.numpy()\n",
    "        start=0\n",
    "        mse_losses={}\n",
    "        mse_losses_flag={}\n",
    "        for w in ner_list:\n",
    "            word=w.word\n",
    "            if word in[',','.','，','。','-','+','?','!']:\n",
    "                continue\n",
    "            flag=w.flag\n",
    "            change_input_ids=input_ids[0]\n",
    "            change_input_ids[start:start+len(word)]=0\n",
    "            new_input_ids=[]\n",
    "            new_input_ids.append(change_input_ids)\n",
    "            new_input_ids=paddle.to_tensor(new_input_ids)\n",
    "            change_logits=model(input_ids=new_input_ids, token_type_ids=segment_ids)\n",
    "            new_start_logits,new_end_logits=change_logits\n",
    "            mse_loss_start=MSE(start_logits.numpy()[0],new_start_logits.numpy()[0])\n",
    "            mse_loss_end=MSE(end_logits.numpy()[0],new_end_logits.numpy()[0])\n",
    "            mse_losses_flag[word]=(mse_loss_start+mse_loss_end)/2\n",
    "            if(flag=='n' or flag=='v'):\n",
    "                mse_losses_flag[word]*=1.1\n",
    "        b = sorted(mse_losses_flag.items(), key=lambda x: x[1],reverse = True)\n",
    "        word_loss.append(b)\n",
    "    pbar.close()\n",
    "    save_word_loss(word_loss, WORD_LOSS_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用于评估的数据\n",
    "评估文件格式要求是3列数据：编号\\t预测答案\\t证据，我们提供了脚本将模型输出结果转成评估所需格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T05:17:28.817649Z",
     "iopub.status.busy": "2022-10-27T05:17:28.817197Z",
     "iopub.status.idle": "2022-10-27T05:17:35.353680Z",
     "shell.execute_reply": "2022-10-27T05:17:35.352715Z",
     "shell.execute_reply.started": "2022-10-27T05:17:28.817622Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data: 1855\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入赋予att方法的权重[0,1] 1\n",
      "请输入满足证据输出的句子分数阈值[0,1] 0.1\n",
      "请输入证据长度范围[0,1] 0.8\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "\n",
    "# 载入未进行文本过滤的测试数据，用于计算TopK\n",
    "original_data_path = WORK_ROOT + 'data/test.txt'\n",
    "original_data_ds, _ = load_test_data(original_data_path)\n",
    "# Re-sort the token index according to their importance scores\n",
    "def resort(index_array, importance_score):\n",
    "    res = sorted([[idx, importance_score[idx]] for idx in index_array], key=lambda x:x[1], reverse=True)\n",
    "    res = [n[0] for n in res]\n",
    "    return res\n",
    "\n",
    "def word_vote(original_data_ds, data_ds, idx, context_sent_pos, importance_score, loss, topk, weight, L, rationale_scale):\n",
    "    assert sum(weight) == 1.0\n",
    "\n",
    "    importance_score -= np.min(importance_score)\n",
    "    assert all(importance_score >= 0.0), importance_score\n",
    "\n",
    "    eval_data_rationale = np.argpartition(importance_score, -topk)[-topk:]\n",
    "    rationale_index_loss = [data_ds[idx]['context'].find(item[0]) for item in loss]\n",
    "    rationale_index_score = list(eval_data_rationale)\n",
    "\n",
    "    indexes = copy.deepcopy(rationale_index_score)\n",
    "    s = '，；：、,;:（）()【】[]{}<>《》“”\"\"‘’#$%^&*`.¥%……&*〉'+\"''\"\n",
    "    for index in indexes:\n",
    "        word = data_ds[idx]['sent_token'][index]\n",
    "        if word in s:\n",
    "            rationale_index_score.remove(index)\n",
    "    \n",
    "    sent_num = len(context_sent_pos[idx]) - 1\n",
    "    vote = np.zeros(sent_num)\n",
    "    score_sum = np.sum(importance_score[rationale_index_score])\n",
    "    j = 0\n",
    "    for pos in rationale_index_score:\n",
    "        for i in range(sent_num):\n",
    "            if context_sent_pos[idx][i]<=pos and context_sent_pos[idx][i+1]>pos:\n",
    "                j += 1\n",
    "                vote[i] += (importance_score[pos] / score_sum) * weight[0]\n",
    "    \n",
    "    loss_list = [item[1] for item in loss]\n",
    "    loss_sum = sum(loss_list)\n",
    "    for index, pos in enumerate(rationale_index_loss):\n",
    "        for i in range(sent_num):\n",
    "            if context_sent_pos[idx][i]<=pos and context_sent_pos[idx][i+1]>pos:\n",
    "                vote[i] += loss_list[index] / loss_sum * weight[1]\n",
    "\n",
    "    sent_index_min_max = np.argsort(vote)\n",
    "    sent_index_max_min = sent_index_min_max[::-1]\n",
    "    '''\n",
    "    if idx == 613:\n",
    "        print(j)\n",
    "        print(score_sum)\n",
    "        print(rationale_index_score)\n",
    "        print(importance_score[rationale_index_score])\n",
    "        print(vote)\n",
    "        print(idx, np.sum(vote), sent_index_max_min)\n",
    "    '''\n",
    "    # assert np.sum(vote) > 0.999, np.sum(vote)\n",
    "\n",
    "    eval_data_rationale = []\n",
    "    v = 0\n",
    "    for sent_index in sent_index_max_min:\n",
    "        v += vote[sent_index]\n",
    "        sent = \"\".join([data_ds[idx]['sent_token'][word_index] for word_index in [i for i in range(context_sent_pos[idx][sent_index], context_sent_pos[idx][sent_index+1]-1)]])\n",
    "        original_sent_pos = original_data_ds[idx]['context'].find(sent[:-1])\n",
    "        eval_data_rationale.extend([i for i in range(original_sent_pos, original_sent_pos+len(sent))])\n",
    "        if len(eval_data_rationale) >= rationale_scale*topk and v>=L:  # ratio范围\n",
    "            break\n",
    "\n",
    "    return eval_data_rationale\n",
    "\n",
    "# 查看中文证据(id, answer, question, chinese_rationales)\n",
    "def get_detail_rationales(interpretation_file, rationale_file, output):\n",
    "    \n",
    "    rationales_fomat = []\n",
    "    with open(rationale_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            id, answer, rationale_id = line.split('\\t')\n",
    "            rationale_id = [int(id) for id in rationale_id.split(',')]\n",
    "            rationales_fomat.append([id, answer, rationale_id])\n",
    "            \n",
    "    data = load_data(interpretation_file)\n",
    "    detail_rationales = ['id\\tanswer\\tquestion\\trationales\\n\\n']\n",
    "    for rationale in rationales_fomat:\n",
    "        id = int(rationale[0])\n",
    "        sent_token = data[id]['sent_token']\n",
    "        answer = rationale[1]\n",
    "        question = data[id]['question']\n",
    "        rationale_id = rationale[2]\n",
    "        # print(id, rationale_id)\n",
    "        rationale_words = \"\".join([sent_token[id] for id in rationale_id])\n",
    "        detail_rationales.append(\"\\t\".join([str(id), answer, question, rationale_words]) + '\\n')\n",
    "    \n",
    "    with open(output, 'w') as f:\n",
    "        f.writelines(detail_rationales)\n",
    "\n",
    "\n",
    "# Post-prepare the result data so that it can be used for the evaluation directly\n",
    "def prepare_eval_data(data, results, paddle_model, method, weight, L, rationale_scale):\n",
    "\n",
    "    res = {}\n",
    "    idx = 0\n",
    "    for data_id, inter_res in zip(data, results):\n",
    "        \n",
    "        # Split importance score vectors for query and title from inter_res.word_attributions\n",
    "        importance_score = np.array(inter_res.word_attributions[1:-1])\n",
    "        loss = word_loss[idx]\n",
    "        # Extract topK importance scores，过滤后的文本长度可能已经小于TopK\n",
    "        topk = min(math.ceil(len(original_data_ds[idx]['sent_token'])*RATIONALE_RATIO), len(importance_score))\n",
    "        \n",
    "        eval_data = {}        \n",
    "        eval_data['id'] = data_id\n",
    "        label = list(inter_res.pred_label)\n",
    "        if int(label[0])>=int(label[1])+1:\n",
    "            eval_data['pred_label'] = 'None'\n",
    "        else:\n",
    "            eval_data['pred_label'] = ''.join(tokenizer.convert_ids_to_tokens(data_ds_new[idx]['input_ids'][int(label[0]):int(label[1])+1]))\n",
    "        if method == 'word_vote':\n",
    "            eval_data['rationale'] = word_vote(original_data_ds, data_ds, idx, context_sent_pos, importance_score, loss, topk, weight, L, rationale_scale)\n",
    "        else:\n",
    "            # Find the token index of the topK importance scores\n",
    "            eval_data['rationale'] = np.argpartition(importance_score, -topk)[-topk:]\n",
    "            # Re-sort the token index according to their importance scores\n",
    "            eval_data['rationale'] = resort(eval_data['rationale'], importance_score)\n",
    "\n",
    "        res[data_id] = eval_data\n",
    "        idx += 1\n",
    "    return res\n",
    "\n",
    "# Generate results for evaluation\n",
    "att_weight = 0.0\n",
    "L = 0.0\n",
    "rationale_scale = 1.0\n",
    "method = 'word_vote'\n",
    "if method == 'word_vote':\n",
    "    att_weight = float(input('请输入赋予att方法的权重[0,1]'))\n",
    "    weight = [att_weight, 1-att_weight]\n",
    "    L = float(input('请输入满足证据输出的句子分数阈值[0,1]'))\n",
    "    rationale_scale = float(input('请输入证据长度范围[0,1]'))\n",
    "\n",
    "predicts= prepare_eval_data(data, align_res, model, method, weight, L, rationale_scale)\n",
    "\n",
    "out_file = open(RESULT_PATH + 'mrc_rationale.txt', 'w')\n",
    "for key in predicts:\n",
    "    out_file.write(str(predicts[key]['id'])+'\\t'+ str(predicts[key]['pred_label'])+'\\t')\n",
    "    for idx in predicts[key]['rationale'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    if predicts[key]['rationale'] != []:\n",
    "        out_file.write(str(predicts[key]['rationale'][-1])+'\\n')\n",
    "    else:\n",
    "        out_file.write(str(0)+'\\n')\n",
    "out_file.close()\n",
    "\n",
    "# 将输出以原文本为模版查看具体的中文证据\n",
    "\n",
    "get_detail_rationales(DATA_PATH+'test.txt', RESULT_PATH+'mrc_rationale.txt', RESULT_PATH+'detail_mrc_rationale.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T05:21:21.391331Z",
     "iopub.status.busy": "2022-10-27T05:21:21.390677Z",
     "iopub.status.idle": "2022-10-27T05:21:21.399748Z",
     "shell.execute_reply": "2022-10-27T05:21:21.398961Z",
     "shell.execute_reply.started": "2022-10-27T05:21:21.391289Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实验超参数记录\n",
    "hyperparameters = {}\n",
    "hyperparameters['version'] = WORK_ROOT[-4:-1]\n",
    "hyperparameters['QA_model'] = MODEL_PATH\n",
    "if 'further' in DATA_FILE:\n",
    "    hyperparameters['test_data'] = {'file': DATA_FILE, 'further select threshold': THRESHOLD}\n",
    "    if 'add' in DATA_FILE:\n",
    "        hyperparameters['test_data']['max_seq_length'] = max_seq_length\n",
    "else:\n",
    "    hyperparameters['test_data'] = DATA_FILE\n",
    "\n",
    "hyperparameters['interpretation'] = {'method': method, 'weight': weight, 'score threhold': L, 'rationale scale': rationale_scale}\n",
    "hyperparameters['result path'] = RESULT_PATH\n",
    "depth = 0\n",
    "\n",
    "def version_record(hyperparameters, record_file, depth):\n",
    "    padding = depth * '\\t'\n",
    "    for parameter in hyperparameters:\n",
    "        if type(hyperparameters[parameter]) == type({}):\n",
    "            record_file.write(f'{padding}{parameter}: \\n')\n",
    "            version_record(hyperparameters[parameter], record_file, depth+1)\n",
    "        else:\n",
    "            record_file.write(f'{padding}{parameter}: {hyperparameters[parameter]}\\n')\n",
    "\n",
    "record_file = open(WORK_ROOT + 'version-record.txt', 'w')\n",
    "version_record(hyperparameters, record_file, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T15:56:22.500344Z",
     "iopub.status.busy": "2022-10-27T15:56:22.499414Z",
     "iopub.status.idle": "2022-10-27T15:56:23.537297Z",
     "shell.execute_reply": "2022-10-27T15:56:23.536474Z",
     "shell.execute_reply.started": "2022-10-27T15:56:22.500301Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "已操作文档数目：: 100%|███████████████████████████████████████| 1855/1855 [00:00<00:00, 3318.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# visual answers\n",
    "import json\n",
    "PUBLIC_DATA_PATH = 'work/public_data/'\n",
    "def generate_refined_dataset(data):\n",
    "    ref_dataset = json.load(open(PUBLIC_DATA_PATH+'refine_dataset.json'))\n",
    "    for examples in ref_dataset:\n",
    "        for example in examples:\n",
    "            data_id = example['id']\n",
    "            example['context'] = data[data_id]['context']\n",
    "            example['sent_token'] = data[data_id]['sent_token']\n",
    "    \n",
    "    with open(DATA_PATH + 'refine_dataset.json','w') as f:\n",
    "        json.dump(ref_dataset,f,indent=4,ensure_ascii=False)\n",
    "\n",
    "\n",
    "refined_dataset = json.load(open(DATA_PATH+'refine_dataset.json'))  # 待修改数据\n",
    "ref_dataset = json.load(open(PUBLIC_DATA_PATH+'refine_dataset.json'))\n",
    "\n",
    "# 可对比查看证据的中文证据转换\n",
    "def insert2refined_dataset(line:str):\n",
    "    data = line.rstrip().split(\"\\t\")\n",
    "    id = data[0]\n",
    "    ans = data[1]\n",
    "    rationale = [int(v) for v in data[2].split(',')]\n",
    "\n",
    "    for i, examples in enumerate(refined_dataset):\n",
    "        for j, example in enumerate(examples):\n",
    "            if str(example['id']) == id:\n",
    "                example['ans'] = ans\n",
    "                example['rationale'] = rationale\n",
    "                example['rationale_text'] = [ref_dataset[i][j]['context'][idx] for idx in rationale]\n",
    "                return\n",
    "\n",
    "generate_refined_dataset(data)\n",
    "\n",
    "pbar = tqdm(total=1855, ncols=100, desc='已操作文档数目：')\n",
    "for line in open(RESULT_PATH + 'mrc_rationale.txt','r').readlines():\n",
    "    pbar.update(1)\n",
    "    insert2refined_dataset(line)\n",
    "pbar.close()\n",
    "\n",
    "\n",
    "with open(RESULT_PATH+'analysis.json','w') as f:\n",
    "    json.dump(refined_dataset,f,indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T17:36:24.326795Z",
     "iopub.status.busy": "2022-10-27T17:36:24.326484Z",
     "iopub.status.idle": "2022-10-27T17:36:24.766628Z",
     "shell.execute_reply": "2022-10-27T17:36:24.765878Z",
     "shell.execute_reply.started": "2022-10-27T17:36:24.326769Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data: 1855\n"
     ]
    }
   ],
   "source": [
    "original_data_path = WORK_ROOT + 'data/test.txt'\n",
    "original_data_ds, data = load_test_data(original_data_path)\n",
    "out_file = open(RESULT_PATH + 'final_result.txt', 'w')\n",
    "for line in open(RESULT_PATH + 'mrc_rationale.txt','r').readlines():\n",
    "    elemts = line.split('\\t')\n",
    "    elemts.insert(2, data[int(elemts[0])]['context'].find(elemts[1]))\n",
    "    out_file.write(f'{elemts[0]}\\t{elemts[1]}\\t{elemts[2]}\\t{elemts[3]}')\n",
    "out_file.close()\n",
    "\n",
    "refined_dataset = json.load(open(RESULT_PATH+'analysis.json'))\n",
    "for examples in refined_dataset:\n",
    "    vote = {}\n",
    "    for example in examples:\n",
    "        if example['ans'] not in vote and example['ans'] not in 'None[SEP]':\n",
    "            vote[example['ans']] = 0\n",
    "        if example['ans'] in vote:\n",
    "            vote[example['ans']] += 1\n",
    "    if vote != {}:\n",
    "        k = max(vote, key=vote.get)  # 找投票最多的答案\n",
    "    else:\n",
    "        k = ''\n",
    "        vote[k] = 1        \n",
    "    if vote[k] > 1:  # 存在\n",
    "        commom_rationale = []\n",
    "        for example in examples:  # 找出该答案的一种证据作为共同证据\n",
    "            if example['ans'] == k:\n",
    "                commom_rationale = example['rationale_text']\n",
    "                break\n",
    "        for example in examples:  # 共享同一答案和证据\n",
    "            example['ans'] = k\n",
    "            rationale_idx = []\n",
    "            rationale_words = []\n",
    "            for rationale in commom_rationale:\n",
    "                if data[int(example['id'])]['context'].find(rationale) != -1:\n",
    "                    idx = data[int(example['id'])]['context'].find(rationale)\n",
    "                    rationale_idx.append(idx)\n",
    "                    rationale_words.append(rationale)\n",
    "            example['rationale'] = rationale_idx\n",
    "            example['rationale_text'] = rationale_words\n",
    "    else:  # 势均力敌的答案，短答案，短证据优先\n",
    "        a_vote = {}\n",
    "        r_vote = {}\n",
    "        for example in examples:\n",
    "            if example['ans'] not in 'None[SEP]':\n",
    "                a_vote[example['ans']] = len(example['ans'])\n",
    "            r_vote[\"\".join(example['rationale_text'])] = len(example['rationale_text'])\n",
    "        if a_vote != {}:\n",
    "            ak = min(a_vote, key=a_vote.get)\n",
    "        else:\n",
    "            ak = ''\n",
    "        rk = min(r_vote, key=r_vote.get)\n",
    "        for example in examples:\n",
    "            example['ans'] = ak\n",
    "            rationale_idx = []\n",
    "            rationale_words = []\n",
    "            for rationale in rk:\n",
    "                if data[int(example['id'])]['context'].find(rationale) != -1:\n",
    "                    idx = data[int(example['id'])]['context'].find(rationale)\n",
    "                    rationale_idx.append(idx)\n",
    "                    rationale_words.append(rationale)\n",
    "            example['rationale'] = rationale_idx\n",
    "            example['rationale_text'] = rationale_words            \n",
    "\n",
    "\n",
    "with open(RESULT_PATH+'analysis_union.json','w') as f:\n",
    "    json.dump(refined_dataset,f,indent=4,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T17:36:28.893226Z",
     "iopub.status.busy": "2022-10-27T17:36:28.892622Z",
     "iopub.status.idle": "2022-10-27T17:36:30.096215Z",
     "shell.execute_reply": "2022-10-27T17:36:30.095219Z",
     "shell.execute_reply.started": "2022-10-27T17:36:28.893197Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "refined_dataset = json.load(open(RESULT_PATH+'analysis_union.json'))\n",
    "out_file = open(RESULT_PATH + 'mrc_rationale_union.txt', 'w')\n",
    "\n",
    "for line in open(RESULT_PATH + 'mrc_rationale.txt','r').readlines():\n",
    "    elemts = line.split('\\t')\n",
    "    for examples in refined_dataset:\n",
    "        for example in examples:\n",
    "            if str(example['id']) == elemts[0]:\n",
    "                out_file.write(f'{elemts[0]}\\t{example[\"ans\"]}\\t')\n",
    "                for idx in example['rationale'][:-1]:\n",
    "                    out_file.write(str(idx)+',')\n",
    "                if example['rationale'] != []:\n",
    "                    out_file.write(str(example['rationale'][-1])+'\\n')\n",
    "                else:\n",
    "                    out_file.write(str(0)+'\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T17:36:30.444276Z",
     "iopub.status.busy": "2022-10-27T17:36:30.443564Z",
     "iopub.status.idle": "2022-10-27T17:36:30.696208Z",
     "shell.execute_reply": "2022-10-27T17:36:30.695496Z",
     "shell.execute_reply.started": "2022-10-27T17:36:30.444248Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查看中文证据(id, answer, question, chinese_rationales)\n",
    "def get_detail_rationales(interpretation_file, rationale_file, output):\n",
    "    \n",
    "    rationales_fomat = []\n",
    "    with open(rationale_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            id, answer, rationale_id = line.split('\\t')\n",
    "            if rationale_id != '\\n':\n",
    "                rationale_id = [int(id) for id in rationale_id.split(',')]\n",
    "            else:\n",
    "                rationale_id = [0]\n",
    "            rationales_fomat.append([id, answer, rationale_id])\n",
    "            \n",
    "    data = load_data(interpretation_file)\n",
    "    detail_rationales = ['id\\tanswer\\tquestion\\trationales\\n\\n']\n",
    "    for rationale in rationales_fomat:\n",
    "        id = int(rationale[0])\n",
    "        sent_token = data[id]['sent_token']\n",
    "        answer = rationale[1]\n",
    "        question = data[id]['question']\n",
    "        rationale_id = rationale[2]\n",
    "        # print(id, rationale_id)\n",
    "        rationale_words = \"\".join([sent_token[id] for id in rationale_id])\n",
    "        detail_rationales.append(\"\\t\".join([str(id), answer, question, rationale_words]) + '\\n')\n",
    "    \n",
    "    with open(output, 'w') as f:\n",
    "        f.writelines(detail_rationales)\n",
    "get_detail_rationales(DATA_PATH+'test.txt', RESULT_PATH+'mrc_rationale_union.txt', RESULT_PATH+'detail_mrc_rationale_union.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
